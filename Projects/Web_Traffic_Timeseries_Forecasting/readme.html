<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h1 id="project-time-series-forecasting-for-wikipedia-daily-visits-dataset">Project: Time Series Forecasting for Wikipedia daily visits dataset</h1>
<h1 id="project-structure">Project Structure</h1>
<ul>
<li>Raw input dataset is taken from <a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/data">kaggle</a>. The data set is larger than github allowed size, I do not have the raw dataset here in github. (file size=284.6MB)</li>
<li><code>notebooks</code> directory has all the notebooks for exploratory data analysis, visualization, modelling and model interpretation.</li>
<li>The project is divided into multiple parts:</li>
<li>ARIMA (Auto Regressive Integrated Moving Average)</li>
<li>VAR (Vector Auto Regression) (for multiple-timesries)</li>
<li>Regressors (Linear, Lasso, Ridge, XGBRegressor)</li>
<li>fbprophet</li>
<li>Deep Learning (LSTM and GRU) (Long Short-term Memory and Gated Recurrent Unit)</li>
</ul>
<h1 id="data-description">Data Description</h1>
<p>Data source: <a href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/data">kaggle</a> The first column is the name of the page and rest 550 columns are visited date.</p>
<pre><code>Original data: train_1.csv
-----------------------------
rows = 145,063
columns = 551
first column = Page
date columns = 2015-07-01, 2015-07-02, ..., 2016-12-31 (550 columns)
file size: 284.6 MB

Date columns:
------------------
Jul/2015 - 31 days
Aug/2015 - 31 days
Sep/2015 - 30 days
Oct/2015 - 31 days
Nov/2015 - 30 days
Dec/2015 - 31 days

Total     : 184 days
Year 2016 : 366 days (leap year)
Total     : 550 days

NOTE:
For this dataset, missing data is represented by 0.

Time series selected for modelling:
ARIMA: most visited page ==&gt; Special:Search_en.wikipedia.org_desktop_all-agents
                              visited = 675606021

VAR: VAR needs correlated times series like opening and closing of stock.
     But, here I took top page per language to see the workings of VAR models
     on wikipedia dataset.

Scikit-learn: For usual regressors like linear, lasso, ridge and for also
              ensemble method xgbregressor, I used most visited page.

fbprophet: For facebook prophet time series modelling module, I used a random
           time series. The page is Now You See me in Spanish Language.

deep-learning: For deep learning algorithms like LSTM and GRU, I used the same
               time series as I used in fbprophet.</code></pre>
<h1 id="best-result-for-prince-musician-timeseries">Best Result for Prince Musician Timeseries</h1>
<p>The best smape for given timeseries was given by xgboost using features from tsfresh.</p>
<h1 id="results-for-prince-musician-timeseries">Results for Prince Musician Timeseries</h1>
<table style="width:76%;">
<colgroup>
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left">Description</th>
<th align="left">MAPE</th>
<th align="left">SMAPE</th>
<th align="left">RMSE</th>
<th align="left">ME</th>
<th align="left">MAE</th>
<th align="left">MPE</th>
<th align="left">CORR</th>
<th align="left">MINMAX</th>
<th align="left">ACF1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">xgb</td>
<td align="left">tsfresh</td>
<td align="left">1</td>
<td align="left">0.6356</td>
<td align="left">337</td>
<td align="left">43</td>
<td align="left">115</td>
<td align="left">0</td>
<td align="left">0.9991</td>
<td align="left">0.0063</td>
<td align="left">-0.2886</td>
</tr>
<tr class="even">
<td align="left">xgb</td>
<td align="left">default</td>
<td align="left">1</td>
<td align="left">1.4308</td>
<td align="left">453</td>
<td align="left">9</td>
<td align="left">224</td>
<td align="left">0</td>
<td align="left">0.9978</td>
<td align="left">0.0141</td>
<td align="left">-0.3971</td>
</tr>
<tr class="odd">
<td align="left">XGBRegressor</td>
<td align="left">default</td>
<td align="left">18</td>
<td align="left">18.2580</td>
<td align="left">4,513</td>
<td align="left">687</td>
<td align="left">2,331</td>
<td align="left">0</td>
<td align="left">0.6643</td>
<td align="left">0.1565</td>
<td align="left">0.1207</td>
</tr>
<tr class="even">
<td align="left">LassoCV</td>
<td align="left">ts_split=3</td>
<td align="left">266</td>
<td align="left">110.8415</td>
<td align="left">25,829</td>
<td align="left">-25,336</td>
<td align="left">25,537</td>
<td align="left">-3</td>
<td align="left">0.5769</td>
<td align="left">0.7062</td>
<td align="left">-0.4231</td>
</tr>
<tr class="odd">
<td align="left">RidgeCV</td>
<td align="left">ts_split=3</td>
<td align="left">261</td>
<td align="left">118.8720</td>
<td align="left">31,289</td>
<td align="left">-15,694</td>
<td align="left">25,228</td>
<td align="left">-2</td>
<td align="left">-0.0255</td>
<td align="left">0.8816</td>
<td align="left">0.6251</td>
</tr>
<tr class="even">
<td align="left">LinearRegression</td>
<td align="left">default</td>
<td align="left">365</td>
<td align="left">135.3122</td>
<td align="left">43,579</td>
<td align="left">-17,255</td>
<td align="left">35,357</td>
<td align="left">-2</td>
<td align="left">-0.1236</td>
<td align="left">1.2735</td>
<td align="left">0.6457</td>
</tr>
<tr class="odd">
<td align="left">LinearRegression</td>
<td align="left">scaled</td>
<td align="left">33,841,890</td>
<td align="left">199.9984</td>
<td align="left">4,378,715,364</td>
<td align="left">-3,640,663,624</td>
<td align="left">3,640,663,624</td>
<td align="left">-338,419</td>
<td align="left">0.5725</td>
<td align="left">1.0000</td>
<td align="left">0.0784</td>
</tr>
<tr class="even">
<td align="left">lstm</td>
<td align="left">lags=2,minmax-scaler</td>
<td align="left">25</td>
<td align="left">24.6649</td>
<td align="left">6,524</td>
<td align="left">353</td>
<td align="left">3,482</td>
<td align="left">-0</td>
<td align="left">0.6507</td>
<td align="left">0.2056</td>
<td align="left">0.6702</td>
</tr>
<tr class="odd">
<td align="left">gru</td>
<td align="left">lags=2</td>
<td align="left">40</td>
<td align="left">53.1378</td>
<td align="left">8,700</td>
<td align="left">5,739</td>
<td align="left">5,739</td>
<td align="left">0</td>
<td align="left">nan</td>
<td align="left">0.4031</td>
<td align="left">0.6727</td>
</tr>
<tr class="even">
<td align="left">gru</td>
<td align="left">lags=2,minmax-scaling</td>
<td align="left">58</td>
<td align="left">83.9143</td>
<td align="left">8,932</td>
<td align="left">7,146</td>
<td align="left">7,192</td>
<td align="left">1</td>
<td align="left">0.5818</td>
<td align="left">0.5815</td>
<td align="left">0.0733</td>
</tr>
<tr class="odd">
<td align="left">lstm</td>
<td align="left">lags=2</td>
<td align="left">99</td>
<td align="left">197.2470</td>
<td align="left">13,684</td>
<td align="left">12,021</td>
<td align="left">12,021</td>
<td align="left">1</td>
<td align="left">0.0502</td>
<td align="left">0.9931</td>
<td align="left">0.6727</td>
</tr>
<tr class="even">
<td align="left">fbprophet</td>
<td align="left">seasonality_after_cap_floor</td>
<td align="left">65</td>
<td align="left">100.4473</td>
<td align="left">9,009</td>
<td align="left">3,603</td>
<td align="left">7,426</td>
<td align="left">0</td>
<td align="left">0.2990</td>
<td align="left">0.5764</td>
<td align="left">0.4837</td>
</tr>
<tr class="odd">
<td align="left">fbprophet</td>
<td align="left">seasonality_before_cap_floor</td>
<td align="left">423</td>
<td align="left">139.6339</td>
<td align="left">54,487</td>
<td align="left">-3,904</td>
<td align="left">44,547</td>
<td align="left">-0</td>
<td align="left">0.1662</td>
<td align="left">2.3876</td>
<td align="left">0.5637</td>
</tr>
<tr class="even">
<td align="left">fbprophet</td>
<td align="left">after_cap_floor</td>
<td align="left">82</td>
<td align="left">147.0780</td>
<td align="left">12,655</td>
<td align="left">7,658</td>
<td align="left">10,089</td>
<td align="left">1</td>
<td align="left">-0.0811</td>
<td align="left">0.7741</td>
<td align="left">0.4794</td>
</tr>
<tr class="odd">
<td align="left">fbprophet</td>
<td align="left">default</td>
<td align="left">437</td>
<td align="left">171.8289</td>
<td align="left">54,429</td>
<td align="left">25,011</td>
<td align="left">48,699</td>
<td align="left">2</td>
<td align="left">-0.2529</td>
<td align="left">3.3718</td>
<td align="left">0.4491</td>
</tr>
<tr class="even">
<td align="left">fbprophet</td>
<td align="left">before_cap_floor</td>
<td align="left">437</td>
<td align="left">171.8289</td>
<td align="left">54,429</td>
<td align="left">25,011</td>
<td align="left">48,699</td>
<td align="left">2</td>
<td align="left">-0.2529</td>
<td align="left">3.3718</td>
<td align="left">0.4491</td>
</tr>
</tbody>
</table>
<h1 id="part-1-data-cleaning-and-feature-engineering">Part 1: Data Cleaning and Feature Engineering</h1>
<p>The data set is super clean, I did not have to do anything. One thing to note is that the nans are represented by 0. This means if some website has 0 visits, it may mean either the acutally 0 persons visited the website or simply the data is not available for that day. The first column is Page and rest 550 columns are dates. For the time series we can create date time for visualization and also for the linear regression modellings.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">df[<span class="st">&#39;year&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.year <span class="co"># yyyy</span>
df[<span class="st">&#39;month&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.month <span class="co"># 1 to 12</span>
df[<span class="st">&#39;day&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.day <span class="co"># 1 to 31</span>
df[<span class="st">&#39;quarter&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.quarter <span class="co"># 1 to 4</span>
df[<span class="st">&#39;dayofweek&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.dayofweek <span class="co"># 0 to 6</span>
df[<span class="st">&#39;dayofyear&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.dayofyear <span class="co"># 1 to 366 (leap year)</span>
df[<span class="st">&#39;weekend&#39;</span>] <span class="op">=</span> ((df[<span class="st">&#39;date&#39;</span>].dt.dayofweek) <span class="op">//</span> <span class="dv">5</span> <span class="op">==</span> <span class="dv">1</span>)
df[<span class="st">&#39;weekday&#39;</span>] <span class="op">=</span> ((df[<span class="st">&#39;date&#39;</span>].dt.dayofweek) <span class="op">//</span> <span class="dv">5</span> <span class="op">!=</span> <span class="dv">1</span>)
df[<span class="st">&#39;day_name&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.day_name() <span class="co"># Monday</span>
df[<span class="st">&#39;month_name&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;date&#39;</span>].dt.month_name() <span class="co"># January</span></code></pre></div>
<h1 id="part-2-data-visualization-and-eda">Part 2: Data visualization and EDA</h1>
<p>For time series visualization, plotly is better tool to visualize the data. For visualization purpose, I looked at only the data of 2016.</p>
<pre><code># of unique pages visited in 2016: 14,506
Top visited page: Special:Search_en.wikipedia.org_desktop_all-agents (675,606,021 visits)</code></pre>
<h1 id="part-3-statistics">Part 3: Statistics</h1>
<p>To fit a linear regression to a given dataset we need the dataset follow some of the assumptions. They are called assumptions of linear regression. Since ARIMA tries to fit the linear regression taking account with autocorrelation with past of itself, still this is a linear regression. We can do some of the linear regression assumptions.</p>
<p>Test of normality: Using Shapiro-Wilk normality test, I found time series is NOT normally distributed.</p>
<p>Test of stationarity: I used Augmented Dickey Fuller test to know whether the given time series is stationary or not. In this particular page, I found the time series is stationary.</p>
<h1 id="part-4-modelling">Part 4: Modelling</h1>
<p>For time series, probably ARIMA (or, SARIMAX) is the most popular algorithm to try on. I used both the usual arima model from <code>statsmodels</code> and also a dedicated library <code>pmdarima</code> to fit the arima model. The details are explained in the notebook.</p>
<p>After doing ARIMA modelling, I was curious what will VAR model do with this wikipedia time series. For VAR method to be used the columns of dataset must be related to each other like opening and closing of the stock. However, just for the purpose of the algorithm implentation and fiddling with the model, I looked at the top 5 pages per language and fitted the model.</p>
<p>Then, I went back in time and wanted to see how will the usual sklearn models like linear regression, lasso and ridge will do with the time series data. I also did some ensemble learning models like xgbregressor. XGBRegressor did pretty good and gave me the SMAPE value of 6.65 for the training data. For a random page (Now You See Me Spanish page), I got the smape of 21.68 on the training data.</p>
<p>For time series forecasting, one of the popular model is prophet open sourced by facebook. This pretty powerful and useful library for the time series modelling.</p>
<p>Then, I wanted to see the usage of deep learning in time series modelling. Particularly, I looked at the models like LSTM and GRU which can remember the past data. I can not use usual CNN since they do not remember the past data points. LSTM did pretty well and gave me smape of 20.34 for the test dataset.</p>
<h1 id="model-evaluation-for-time-series">Model Evaluation for Time Series</h1>
<p>One of the most popular metric to determine the performance of time series model is SMAPE (Symmetric Mean Absolute Percentage Error).</p>
<p>The formula for SMAPE (Symmetric Mean Absolute Percentage Error) is given below:</p>
<pre><code>SMAPE =  200 * mean   abs(A-F)
                      -----------------
                      abs(A) + abs(F)

SMAPE lies between 0 and 200, 0 is best and 200 is worst.</code></pre>
<p><br /><span class="math display">$$
S M A P E=\frac{100 \%}{n} \sum_{t=1}^{n} \frac{\left|F_{t}-A_{t}\right|}{\left(\left|F_{t}\right|+\left|A_{t}\right|\right) / 2}
$$</span><br /></p>
<p>Where, F is forecast and A is the actual value of time series at given time t.</p>
<p>Python implementation:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> smape(A, F):
    F <span class="op">=</span> A[:<span class="bu">len</span>(A)]
    <span class="cf">return</span> ( <span class="fl">200.0</span><span class="op">/</span><span class="bu">len</span>(A) <span class="op">*</span> np.<span class="bu">sum</span>(  np.<span class="bu">abs</span>(F <span class="op">-</span> A) <span class="op">/</span>
                                  (np.<span class="bu">abs</span>(A) <span class="op">+</span> np.<span class="bu">abs</span>(F) <span class="op">+</span> np.finfo(<span class="bu">float</span>).eps))
           )</code></pre></div>
<p>Despite the name Symmetric, the smape is not actually symmetric. Take this example from <a href="https://www.wikiwand.com/en/Symmetric_mean_absolute_percentage_error">wikipedia</a> for an example:</p>
<p>The SMAPE is not symmetric since over- and under-forecasts are not treated equally. This is illustrated by the following example by applying the SMAPE formula:</p>
<pre><code>Over-forecasting : At = 100 and Ft = 110 gives SMAPE = 4.76%
Under-forecasting: At = 100 and Ft = 90  gives SMAPE = 5.26%.</code></pre>
<h1 id="useful-resources-for-timeseries-analysis">Useful Resources for Timeseries Analysis</h1>
<ul>
<li>https://github.com/MaxBenChrist/awesome_time_series_in_python</li>
</ul>
</body>
</html>
