# Q: NLP Based Clinical Text classification

Can you explain how you did this project.

Absolutely! Here's a detailed yet interview-ready explanation of the **NLP-Based Clinical Text Classification** project you did at **AmerisourceBergen**, tailored to highlight both technical and business impact:



### ðŸ§  **NLP-Based Clinical Text Classification**

#### ðŸŽ¯ **Objective:**

Segment and classify unstructured patient "chatter" (messages, emails, case notes) into actionable categories such as medication issues, appointment requests, insurance questions, etc., to **prioritize support tickets** and enable faster response from patient service teams.



### ðŸ§© **Step-by-Step Approach**

#### 1. **Data Preprocessing**

* Handled messy healthcare text: acronyms, typos, and medical slang.
* Applied:

  * Text cleaning (lowercasing, stop word removal, regex)
  * Lemmatization using **spaCy**
  * De-identification (removed PII for compliance)

#### 2. **Text Vectorization**

* Tried multiple embeddings:

  * **Bag of Words**, **TF-IDF** as baselines
  * **BERT**, **T5**, and **BioBERT** for contextual embeddings
* Found BERT-based embeddings captured nuances in medical phrasing better.

#### 3. **Modeling**

* Fine-tuned transformer models like **BERT** on labeled clinical notes using `transformers` (HuggingFace).
* Compared performance with:

  * Logistic Regression
  * Random Forest
  * Support Vector Machines (SVM)
* Best results came from **BERT + classification head** (softmax layer).

#### 4. **Evaluation**

* Used stratified train-test split and cross-validation.
* Key metrics:

  * **F1-score** (handled class imbalance)
  * **Precision/Recall** for critical categories like medication or emergency
* Achieved **F1-score of \~0.91** on validation set.

#### 5. **Deployment & Use Case**

* Exposed model via **FastAPI**.
* Connected it to the customer service toolchain to **auto-tag incoming tickets**.
* Enabled **real-time triaging** and significantly reduced manual workload.



### ðŸ“¦ **Tools & Tech Used:**

* `transformers`, `scikit-learn`, `nltk`, `spaCy`, `torch`
* FastAPI for model serving
* Azure DevOps for deployment pipelines



### ðŸ“ˆ **Impact:**

* Automated classification reduced ticket triage time by **40%+**.
* Ensured high precision for critical classes (e.g., medication-related issues).
* Improved patient response times and reduced SLA violations.



Let me know if you'd like a one-liner for your resume or a diagram for interviews.

Certainly!



**Q: Can you explain how you did the NLP-based clinical text classification project?**

**Answer:**

Yes, in my role at AmerisourceBergen, I led an NLP-based clinical text classification project focused on analyzing unstructured patient chatter data to enhance prioritization and response times for healthcare teams.

We began by collecting raw clinical notes and chat transcripts. These texts were highly noisy, often containing misspellings, acronyms, and domain-specific slang. I applied robust preprocessing pipelines including regex-based cleaning, token normalization, and abbreviation expansion. For example, "pt c/o HA x3d" was normalized to "patient complains of headache for 3 days".

To classify the texts into predefined categories (e.g., urgent care needed, medication inquiry, appointment scheduling), I used a hybrid modeling approach:

1. **Few-Shot Learning with GPT-3.5**:
   We leveraged GPT-3.5 via OpenAI's API in a few-shot prompt engineering setup. This allowed us to quickly build a baseline classifier without extensive labeled data. For instance, we used prompt templates like:

   ```
   Text: "I've been feeling dizzy since I took my meds."
   Category: Medication Side Effects

   Text: "Can I reschedule my appointment for next week?"
   Category: Scheduling
   ```

   This approach helped us triage messages before full fine-tuning and gave us explainability via GPT's generated rationales.

2. **Fine-Tuned BERT/T5 Models**:
   After establishing the pipeline, I fine-tuned transformer-based models like BERT and T5 using our internal labeled dataset. We leveraged `transformers` and `sklearn` to evaluate metrics like F1-score and recall, achieving significant gains over the GPT-3 baseline.

3. **Medical-Specific Embeddings**:
   We also experimented with BioBERT and ClinicalBERT embeddings to better capture domain-specific terminology. These models were more robust to healthcare-specific phrases and outperformed vanilla BERT in classification accuracy by \~7%.

4. **Deployment & Integration**:
   The final model was containerized using Docker and exposed via a REST API with FastAPI. We integrated this with internal dashboards using PowerBI so that support teams could view flagged messages in near real-time.

5. **Explainability & Ethics**:
   I ensured interpretability by attaching model-generated explanations to each prediction using SHAP and GPT's own text rationales. Additionally, we masked all PII and followed HIPAA-compliant data governance protocols.

**Outcome**:
This system significantly reduced manual triage workload and improved the average response time to critical patient queries by over 30%. It also enabled downstream analytics on patient concerns, driving strategic improvements in service delivery.

