
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Timeseries Quiz</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <div class="container">
        <h1>Timeseries Quiz</h1>
        <div class="quiz-container">
    
            <div class="question" id="q1">
                <h3>Question 1: What does the 'AR' component in ARIMA represent, and how does it capture patterns in time series data?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q1" id="q1o0" value="0"><label for="q1o0">Autoregressive - using past values to predict future values</label></div>
<div class="option"><input type="radio" name="q1" id="q1o1" value="1"><label for="q1o1">Auto Recursive - using recursive algorithms for prediction</label></div>
<div class="option"><input type="radio" name="q1" id="q1o2" value="2"><label for="q1o2">Autonomously Restrictive - restricting variables autonomously</label></div>
<div class="option"><input type="radio" name="q1" id="q1o3" value="3"><label for="q1o3">Average Residuals - averaging the residual errors</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Autoregressive - using past values to predict future values</p>
                    <p><strong>Explanation:</strong> The 'AR' in ARIMA stands for Autoregressive, which means the model uses past values of the time series to predict future values. Specifically, an AR(p) model uses p previous time steps as predictors. For example, in an AR(2) model, the current value is predicted using a linear combination of the previous two values, plus an error term. This component is particularly useful for capturing momentum or inertia in time series where recent values influence future values.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q2">
                <h3>Question 2: What does the 'I' component in ARIMA represent, and why is it necessary?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q2" id="q2o0" value="0"><label for="q2o0">Integrated - differencing to achieve stationarity</label></div>
<div class="option"><input type="radio" name="q2" id="q2o1" value="1"><label for="q2o1">Interpolated - filling in missing values</label></div>
<div class="option"><input type="radio" name="q2" id="q2o2" value="2"><label for="q2o2">Independent - ensuring variables are independent</label></div>
<div class="option"><input type="radio" name="q2" id="q2o3" value="3"><label for="q2o3">Incremental - adding incremental changes to the model</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Integrated - differencing to achieve stationarity</p>
                    <p><strong>Explanation:</strong> The 'I' in ARIMA stands for Integrated, which refers to differencing the time series to achieve stationarity. Many time series have trends or seasonal patterns that make them non-stationary. The 'd' parameter in ARIMA(p,d,q) indicates how many times the data needs to be differenced to achieve stationarity. For example, if d=1, we take the difference between consecutive observations. This transformation is necessary because ARIMA models assume the underlying process is stationary, meaning its statistical properties do not change over time.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q3">
                <h3>Question 3: What does the 'MA' component in ARIMA represent, and how does it differ from AR?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q3" id="q3o0" value="0"><label for="q3o0">Moving Average - using past forecast errors in the model</label></div>
<div class="option"><input type="radio" name="q3" id="q3o1" value="1"><label for="q3o1">Mean Adjustment - adjusting the mean over time</label></div>
<div class="option"><input type="radio" name="q3" id="q3o2" value="2"><label for="q3o2">Maximum Amplitude - capturing amplitude changes</label></div>
<div class="option"><input type="radio" name="q3" id="q3o3" value="3"><label for="q3o3">Multiple Analysis - analyzing multiple factors simultaneously</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Moving Average - using past forecast errors in the model</p>
                    <p><strong>Explanation:</strong> The 'MA' in ARIMA stands for Moving Average, which incorporates past forecast errors (residuals) into the model rather than past values of the time series itself. An MA(q) model uses the previous q forecast errors as predictors. This differs fundamentally from AR, which uses the actual past values. MA components capture the short-term reactions to past shocks or random events in the system. For example, an MA(1) model would use the forecast error from the previous time step to adjust the current prediction.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q4">
                <h3>Question 4: How do you interpret the parameters p, d, and q in ARIMA(p,d,q)?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q4" id="q4o0" value="0"><label for="q4o0">p = AR order, d = differencing order, q = MA order</label></div>
<div class="option"><input type="radio" name="q4" id="q4o1" value="1"><label for="q4o1">p = prediction horizon, d = data points, q = quality metric</label></div>
<div class="option"><input type="radio" name="q4" id="q4o2" value="2"><label for="q4o2">p = precision factor, d = decay rate, q = quantile value</label></div>
<div class="option"><input type="radio" name="q4" id="q4o3" value="3"><label for="q4o3">p = periodicity, d = dimension reduction, q = querying frequency</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> p = AR order, d = differencing order, q = MA order</p>
                    <p><strong>Explanation:</strong> In ARIMA(p,d,q), p represents the order of the autoregressive (AR) component, indicating how many lagged values of the series are included in the model. A higher p means more past values are used for prediction. The parameter d represents the degree of differencing required to make the series stationary, with d=1 meaning first difference, d=2 meaning second difference, etc. Finally, q is the order of the moving average (MA) component, indicating how many lagged forecast errors are included in the model. Together, these parameters define the structure of the ARIMA model and must be carefully selected based on the characteristics of the time series.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q5">
                <h3>Question 5: What is the key assumption that must be satisfied before applying ARIMA models?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q5" id="q5o0" value="0"><label for="q5o0">The time series must be stationary</label></div>
<div class="option"><input type="radio" name="q5" id="q5o1" value="1"><label for="q5o1">The time series must have missing values imputed</label></div>
<div class="option"><input type="radio" name="q5" id="q5o2" value="2"><label for="q5o2">The time series must follow a normal distribution</label></div>
<div class="option"><input type="radio" name="q5" id="q5o3" value="3"><label for="q5o3">The time series must have equal intervals between observations</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> The time series must be stationary</p>
                    <p><strong>Explanation:</strong> The fundamental assumption for ARIMA models is that the time series is stationary or can be made stationary through differencing. A stationary time series has constant mean, variance, and autocorrelation structure over time. Without stationarity, the model cannot reliably learn patterns from the data. This is why the 'I' (Integrated) component exists in ARIMA - to transform non-stationary data through differencing. Analysts typically use statistical tests like the Augmented Dickey-Fuller (ADF) test to check for stationarity before applying ARIMA models.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q6">
                <h3>Question 6: How can you determine the appropriate values for p and q in an ARIMA(p,d,q) model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q6" id="q6o0" value="0"><label for="q6o0">By examining ACF and PACF plots</label></div>
<div class="option"><input type="radio" name="q6" id="q6o1" value="1"><label for="q6o1">By using cross-validation only</label></div>
<div class="option"><input type="radio" name="q6" id="q6o2" value="2"><label for="q6o2">By checking the kurtosis and skewness</label></div>
<div class="option"><input type="radio" name="q6" id="q6o3" value="3"><label for="q6o3">By analyzing the histogram of the data</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> By examining ACF and PACF plots</p>
                    <p><strong>Explanation:</strong> The appropriate values for p and q in an ARIMA model can be determined by examining the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots of the stationary time series. For identifying the AR order (p), look for significant spikes in the PACF that cut off after lag p. For the MA order (q), look for significant spikes in the ACF that cut off after lag q. Additionally, information criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) can be used to compare different model specifications and select the best combination of parameters.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q7">
                <h3>Question 7: What is the purpose of the Augmented Dickey-Fuller (ADF) test in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q7" id="q7o0" value="0"><label for="q7o0">To test for stationarity</label></div>
<div class="option"><input type="radio" name="q7" id="q7o1" value="1"><label for="q7o1">To calculate forecast accuracy</label></div>
<div class="option"><input type="radio" name="q7" id="q7o2" value="2"><label for="q7o2">To detect seasonality</label></div>
<div class="option"><input type="radio" name="q7" id="q7o3" value="3"><label for="q7o3">To identify outliers</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> To test for stationarity</p>
                    <p><strong>Explanation:</strong> The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or not. The null hypothesis of the test is that the time series contains a unit root, implying it is non-stationary. If the p-value from the test is less than the significance level (typically 0.05), we reject the null hypothesis and conclude that the series is stationary. This test is crucial before applying ARIMA models because stationarity is a key assumption. The test includes lags of the differenced series to account for serial correlation, making it more robust than the simple Dickey-Fuller test.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q8">
                <h3>Question 8: What is the difference between ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function)?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q8" id="q8o0" value="0"><label for="q8o0">ACF measures correlation between series and lagged values accounting for intermediate lags, PACF removes indirect correlation effects</label></div>
<div class="option"><input type="radio" name="q8" id="q8o1" value="1"><label for="q8o1">ACF is for AR models only, PACF is for MA models only</label></div>
<div class="option"><input type="radio" name="q8" id="q8o2" value="2"><label for="q8o2">ACF works on raw data, PACF requires differenced data</label></div>
<div class="option"><input type="radio" name="q8" id="q8o3" value="3"><label for="q8o3">ACF is a graphical technique, PACF is a numerical technique</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> ACF measures correlation between series and lagged values accounting for intermediate lags, PACF removes indirect correlation effects</p>
                    <p><strong>Explanation:</strong> The ACF (Autocorrelation Function) measures the correlation between a time series and its lagged values, including both direct and indirect effects. It shows the correlation at each lag without controlling for correlations at shorter lags. In contrast, the PACF (Partial Autocorrelation Function) measures the correlation between a time series and its lagged values while controlling for the values of the time series at all shorter lags. This effectively removes the indirect correlation effects, showing only the direct relationship between observations separated by a specific lag. ACF helps identify MA(q) order, while PACF helps identify AR(p) order in ARIMA modeling.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q9">
                <h3>Question 9: What additional component does SARIMAX add compared to ARIMA?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q9" id="q9o0" value="0"><label for="q9o0">Seasonal components and exogenous variables</label></div>
<div class="option"><input type="radio" name="q9" id="q9o1" value="1"><label for="q9o1">Square root transformation capabilities</label></div>
<div class="option"><input type="radio" name="q9" id="q9o2" value="2"><label for="q9o2">Sigmoid activation functions</label></div>
<div class="option"><input type="radio" name="q9" id="q9o3" value="3"><label for="q9o3">Smoothing parameters for exponential weighting</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Seasonal components and exogenous variables</p>
                    <p><strong>Explanation:</strong> SARIMAX (Seasonal AutoRegressive Integrated Moving Average with eXogenous factors) extends ARIMA by adding two important capabilities. First, it incorporates seasonal components, allowing the model to capture repeating patterns that occur at fixed intervals (like daily, weekly, or yearly seasonality). The seasonal component is specified with parameters (P,D,Q)m, where m is the seasonal period. Second, SARIMAX allows for exogenous variables (the 'X' part), which are external factors that can influence the time series but are not part of the series itself. These could include variables like temperature affecting energy consumption, or promotions affecting sales. This makes SARIMAX much more versatile than standard ARIMA for real-world applications with seasonal patterns and external influences.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q10">
                <h3>Question 10: How are seasonality parameters represented in a SARIMA model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q10" id="q10o0" value="0"><label for="q10o0">As (P,D,Q)m where m is the seasonal period</label></div>
<div class="option"><input type="radio" name="q10" id="q10o1" value="1"><label for="q10o1">As a sine wave with amplitude A and period T</label></div>
<div class="option"><input type="radio" name="q10" id="q10o2" value="2"><label for="q10o2">As a separate time series added to the main series</label></div>
<div class="option"><input type="radio" name="q10" id="q10o3" value="3"><label for="q10o3">As a scaling factor applied to the ACF</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> As (P,D,Q)m where m is the seasonal period</p>
                    <p><strong>Explanation:</strong> In a SARIMA (Seasonal ARIMA) model, seasonality parameters are represented as (P,D,Q)m, where P is the seasonal autoregressive order, D is the seasonal differencing order, Q is the seasonal moving average order, and m is the number of periods in each season (the seasonal period). For example, in monthly data with yearly seasonality, m would be 12. In a SARIMA(1,1,1)(1,1,1)12 model, the non-seasonal components are (1,1,1) and the seasonal components are (1,1,1)12. The seasonal components operate at lag m, 2m, etc., capturing patterns that repeat every m periods. Seasonal differencing (D) involves subtracting the value from m periods ago, helping to remove seasonal non-stationarity.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q11">
                <h3>Question 11: What does it mean when we say a time series exhibits 'stationarity'?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q11" id="q11o0" value="0"><label for="q11o0">Its statistical properties remain constant over time</label></div>
<div class="option"><input type="radio" name="q11" id="q11o1" value="1"><label for="q11o1">It has no missing values</label></div>
<div class="option"><input type="radio" name="q11" id="q11o2" value="2"><label for="q11o2">It shows strong seasonality</label></div>
<div class="option"><input type="radio" name="q11" id="q11o3" value="3"><label for="q11o3">It has been sampled at regular intervals</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Its statistical properties remain constant over time</p>
                    <p><strong>Explanation:</strong> A stationary time series has statistical properties that remain constant over time. Specifically, it has a constant mean, constant variance, and a constant autocorrelation structure. This means the process generating the time series is in statistical equilibrium. Stationarity is a crucial assumption for many time series models, including ARIMA, because it ensures that patterns learned from historical data will continue to be valid in the future. Non-stationary series might have trends (changing mean) or heteroscedasticity (changing variance), which can lead to unreliable forecasts if not properly addressed through transformations like differencing or variance stabilization.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q12">
                <h3>Question 12: What is the purpose of differencing in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q12" id="q12o0" value="0"><label for="q12o0">To remove trends and achieve stationarity</label></div>
<div class="option"><input type="radio" name="q12" id="q12o1" value="1"><label for="q12o1">To smooth out random fluctuations</label></div>
<div class="option"><input type="radio" name="q12" id="q12o2" value="2"><label for="q12o2">To interpolate missing values</label></div>
<div class="option"><input type="radio" name="q12" id="q12o3" value="3"><label for="q12o3">To standardize the scale of the data</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> To remove trends and achieve stationarity</p>
                    <p><strong>Explanation:</strong> Differencing in time series analysis involves computing the differences between consecutive observations. The primary purpose is to remove trends and achieve stationarity, which is a key requirement for ARIMA modeling. First-order differencing (d=1) can eliminate linear trends by calculating Yt - Yt-1. If the series still shows non-stationarity after first differencing, second-order differencing (d=2) can be applied to remove quadratic trends. However, over-differencing can introduce unnecessary complexity and artificial patterns, so it's important to use statistical tests like the ADF test to determine the appropriate level of differencing needed.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q13">
                <h3>Question 13: What is seasonal differencing and when should it be applied?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q13" id="q13o0" value="0"><label for="q13o0">Calculating differences between observations separated by the seasonal period, applied when there's seasonal non-stationarity</label></div>
<div class="option"><input type="radio" name="q13" id="q13o1" value="1"><label for="q13o1">Calculating differences between adjacent seasons, applied to remove annual trends</label></div>
<div class="option"><input type="radio" name="q13" id="q13o2" value="2"><label for="q13o2">Finding the mean difference between seasons, applied to normalize seasonal data</label></div>
<div class="option"><input type="radio" name="q13" id="q13o3" value="3"><label for="q13o3">Converting seasons to binary variables, applied for classification models</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Calculating differences between observations separated by the seasonal period, applied when there's seasonal non-stationarity</p>
                    <p><strong>Explanation:</strong> Seasonal differencing involves calculating differences between observations separated by the seasonal period (e.g., 12 months for monthly data with yearly seasonality). It's represented by the D parameter in SARIMA models and is applied when the time series exhibits seasonal non-stationarity, meaning the seasonal pattern changes over time. For example, with monthly data, seasonal differencing would compute Yt - Yt-12. This helps remove repeating seasonal patterns just as regular differencing removes trends. You should apply seasonal differencing when visual inspection shows persistent seasonal patterns after regular differencing, or when seasonal unit root tests indicate seasonal non-stationarity.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q14">
                <h3>Question 14: What are residuals in the context of ARIMA modeling, and why are they important?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q14" id="q14o0" value="0"><label for="q14o0">The differences between observed and predicted values, important for diagnostic checking</label></div>
<div class="option"><input type="radio" name="q14" id="q14o1" value="1"><label for="q14o1">The remaining trends after differencing, important for model specification</label></div>
<div class="option"><input type="radio" name="q14" id="q14o2" value="2"><label for="q14o2">The seasonal components not captured by the model, important for seasonal adjustment</label></div>
<div class="option"><input type="radio" name="q14" id="q14o3" value="3"><label for="q14o3">The exogenous variables not included in the model, important for feature selection</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> The differences between observed and predicted values, important for diagnostic checking</p>
                    <p><strong>Explanation:</strong> In ARIMA modeling, residuals are the differences between the observed values and the values predicted by the model. They represent the part of the data that the model couldn't explain. Residuals are crucial for diagnostic checking because a well-fitted ARIMA model should have residuals that resemble white noise - they should be uncorrelated, have zero mean, constant variance, and follow a normal distribution. If patterns remain in the residuals, it suggests the model hasn't captured all the systematic information in the time series. Common residual diagnostics include ACF/PACF plots of residuals, the Ljung-Box test for autocorrelation, and Q-Q plots for normality checking.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q15">
                <h3>Question 15: What does the Ljung-Box test evaluate in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q15" id="q15o0" value="0"><label for="q15o0">Whether residuals exhibit autocorrelation</label></div>
<div class="option"><input type="radio" name="q15" id="q15o1" value="1"><label for="q15o1">Whether the time series is stationary</label></div>
<div class="option"><input type="radio" name="q15" id="q15o2" value="2"><label for="q15o2">Whether the model has the correct number of parameters</label></div>
<div class="option"><input type="radio" name="q15" id="q15o3" value="3"><label for="q15o3">Whether the series has significant seasonality</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Whether residuals exhibit autocorrelation</p>
                    <p><strong>Explanation:</strong> The Ljung-Box test is a statistical test used to evaluate whether residuals from a time series model exhibit autocorrelation. The null hypothesis is that the residuals are independently distributed (i.e., no autocorrelation). If the p-value is less than the significance level (typically 0.05), we reject the null hypothesis and conclude that the residuals contain significant autocorrelation, suggesting the model hasn't captured all the patterns in the data. The test examines multiple lags simultaneously, making it more comprehensive than just looking at individual autocorrelation values. A good ARIMA model should have residuals that pass the Ljung-Box test, indicating they approximate white noise.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q16">
                <h3>Question 16: What is the primary difference between ARMA and ARIMA models?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q16" id="q16o0" value="0"><label for="q16o0">ARIMA includes differencing for non-stationary data, while ARMA requires stationary data</label></div>
<div class="option"><input type="radio" name="q16" id="q16o1" value="1"><label for="q16o1">ARIMA works with continuous data, ARMA only works with discrete data</label></div>
<div class="option"><input type="radio" name="q16" id="q16o2" value="2"><label for="q16o2">ARIMA includes exogenous variables, ARMA does not</label></div>
<div class="option"><input type="radio" name="q16" id="q16o3" value="3"><label for="q16o3">ARIMA can handle missing values, ARMA cannot</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> ARIMA includes differencing for non-stationary data, while ARMA requires stationary data</p>
                    <p><strong>Explanation:</strong> The primary difference between ARMA (AutoRegressive Moving Average) and ARIMA (AutoRegressive Integrated Moving Average) models is that ARIMA includes a differencing step (the 'I' component) to handle non-stationary data. ARMA models combine autoregressive (AR) and moving average (MA) components but assume that the time series is already stationary. ARIMA extends this by first differencing the data d times to achieve stationarity before applying the ARMA model. This makes ARIMA more versatile for real-world time series that often contain trends. Essentially, an ARIMA(p,d,q) model is equivalent to applying an ARMA(p,q) model to a time series after differencing it d times.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q17">
                <h3>Question 17: What is meant by the 'order of integration' in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q17" id="q17o0" value="0"><label for="q17o0">The number of times a series needs to be differenced to achieve stationarity</label></div>
<div class="option"><input type="radio" name="q17" id="q17o1" value="1"><label for="q17o1">The number of times a series needs to be smoothed to remove noise</label></div>
<div class="option"><input type="radio" name="q17" id="q17o2" value="2"><label for="q17o2">The number of observations required for reliable forecasting</label></div>
<div class="option"><input type="radio" name="q17" id="q17o3" value="3"><label for="q17o3">The number of exogenous variables included in the model</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> The number of times a series needs to be differenced to achieve stationarity</p>
                    <p><strong>Explanation:</strong> The 'order of integration' refers to the number of times a time series needs to be differenced to achieve stationarity. It's represented by the parameter d in ARIMA(p,d,q) models. A series that requires differencing once (d=1) to become stationary is said to be integrated of order 1, or I(1). Similarly, a series requiring two differences is I(2). A naturally stationary series is I(0). The concept is important because it quantifies how persistent trends are in the data. Most economic and business time series are I(1), meaning they have stochastic trends that can be removed with first differencing. The order of integration can be determined using unit root tests like the Augmented Dickey-Fuller test.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q18">
                <h3>Question 18: What is the purpose of the Box-Jenkins methodology in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q18" id="q18o0" value="0"><label for="q18o0">A systematic approach to identify, estimate, and validate ARIMA models</label></div>
<div class="option"><input type="radio" name="q18" id="q18o1" value="1"><label for="q18o1">A transformation technique to normalize skewed time series data</label></div>
<div class="option"><input type="radio" name="q18" id="q18o2" value="2"><label for="q18o2">A diagnostic test for heteroscedasticity in residuals</label></div>
<div class="option"><input type="radio" name="q18" id="q18o3" value="3"><label for="q18o3">A method to decompose time series into trend, seasonal, and residual components</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> A systematic approach to identify, estimate, and validate ARIMA models</p>
                    <p><strong>Explanation:</strong> The Box-Jenkins methodology is a systematic approach to identify, estimate, and validate ARIMA models for time series forecasting. It consists of three main stages: identification, estimation, and diagnostic checking. In the identification stage, you determine appropriate values for p, d, and q by analyzing ACF/PACF plots and using stationarity tests. In the estimation stage, you fit the selected ARIMA model to the data and estimate its parameters. In the diagnostic checking stage, you analyze residuals to ensure they resemble white noise and refine the model if needed. Box-Jenkins emphasizes iterative model building, where you cycle through these stages until you find an adequate model. This methodical approach helps ensure that the final model captures the data's patterns efficiently.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q19">
                <h3>Question 19: What information criterion is commonly used to select between different ARIMA models?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q19" id="q19o0" value="0"><label for="q19o0">AIC (Akaike Information Criterion)</label></div>
<div class="option"><input type="radio" name="q19" id="q19o1" value="1"><label for="q19o1">R-squared value</label></div>
<div class="option"><input type="radio" name="q19" id="q19o2" value="2"><label for="q19o2">Mean Absolute Error (MAE)</label></div>
<div class="option"><input type="radio" name="q19" id="q19o3" value="3"><label for="q19o3">Standard Error of the regression</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> AIC (Akaike Information Criterion)</p>
                    <p><strong>Explanation:</strong> The AIC (Akaike Information Criterion) is commonly used to select between different ARIMA models. It balances model fit against complexity by penalizing models with more parameters. The formula is AIC = -2log(L) + 2k, where L is the likelihood of the model and k is the number of parameters. A lower AIC value indicates a better model. When comparing ARIMA models with different p, d, and q values, analysts typically choose the model with the lowest AIC. Other similar criteria include BIC (Bayesian Information Criterion), which penalizes model complexity more heavily. These criteria help prevent overfitting by ensuring that additional parameters are only included if they substantially improve the model's fit to the data.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q20">
                <h3>Question 20: In the context of ARIMA residual analysis, what should a Q-Q plot ideally show?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q20" id="q20o0" value="0"><label for="q20o0">Points falling approximately along a straight line, indicating normally distributed residuals</label></div>
<div class="option"><input type="radio" name="q20" id="q20o1" value="1"><label for="q20o1">Points forming a horizontal band, indicating homoscedasticity</label></div>
<div class="option"><input type="radio" name="q20" id="q20o2" value="2"><label for="q20o2">Points showing no pattern, indicating randomness</label></div>
<div class="option"><input type="radio" name="q20" id="q20o3" value="3"><label for="q20o3">Points clustered around zero, indicating unbiased estimation</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Points falling approximately along a straight line, indicating normally distributed residuals</p>
                    <p><strong>Explanation:</strong> In ARIMA residual analysis, a Q-Q (Quantile-Quantile) plot should ideally show points falling approximately along a straight line. This indicates that the residuals follow a normal distribution, which is an assumption for valid statistical inference in ARIMA modeling. The Q-Q plot compares the quantiles of the residuals against the quantiles of a theoretical normal distribution. Deviations from the straight line suggest non-normality: a sigmoidal pattern indicates skewness, while an S-shaped curve suggests heavy or light tails compared to a normal distribution. Serious deviations might indicate model misspecification or the presence of outliers that could affect the reliability of confidence intervals and hypothesis tests for the model parameters.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q21">
                <h3>Question 21: What is the meaning of the 'exogenous variables' in the context of SARIMAX models?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q21" id="q21o0" value="0"><label for="q21o0">External predictor variables that influence the time series but are not influenced by it</label></div>
<div class="option"><input type="radio" name="q21" id="q21o1" value="1"><label for="q21o1">Random error terms that follow an external probability distribution</label></div>
<div class="option"><input type="radio" name="q21" id="q21o2" value="2"><label for="q21o2">Extraordinary observations that are treated as outliers</label></div>
<div class="option"><input type="radio" name="q21" id="q21o3" value="3"><label for="q21o3">Exponential growth factors in the time series</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> External predictor variables that influence the time series but are not influenced by it</p>
                    <p><strong>Explanation:</strong> In SARIMAX models, exogenous variables (the 'X' part) are external predictor variables that influence the time series being modeled but are not influenced by it. These are independent variables that provide additional information beyond what's contained in the past values of the time series itself. For example, when forecasting electricity demand, temperature might be an exogenous variable since it affects demand but isn't affected by it. Unlike the autoregressive components that use the series' own past values, exogenous variables inject outside information into the model. This can significantly improve forecast accuracy when the time series is known to be affected by measurable external factors. Mathematically, exogenous variables enter the SARIMAX equation as a regression component.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q22">
                <h3>Question 22: Why might you perform a Box-Cox transformation before applying an ARIMA model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q22" id="q22o0" value="0"><label for="q22o0">To stabilize variance and make the data more normally distributed</label></div>
<div class="option"><input type="radio" name="q22" id="q22o1" value="1"><label for="q22o1">To remove seasonality from the data</label></div>
<div class="option"><input type="radio" name="q22" id="q22o2" value="2"><label for="q22o2">To reduce the impact of outliers</label></div>
<div class="option"><input type="radio" name="q22" id="q22o3" value="3"><label for="q22o3">To convert the time series to a stationary process</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> To stabilize variance and make the data more normally distributed</p>
                    <p><strong>Explanation:</strong> A Box-Cox transformation is often performed before applying an ARIMA model to stabilize variance and make the data more normally distributed. Many time series exhibit heteroscedasticity (changing variance over time) or skewness, which can violate ARIMA assumptions. The Box-Cox transformation is a family of power transformations defined by the parameter 位: when 位=0, it's equivalent to a log transformation; when 位=1, it's essentially the original data (with a shift). The optimal 位 value can be determined by maximizing the log-likelihood function. This transformation helps make the time series' variance more constant across time and its distribution more symmetric, leading to more reliable parameter estimates and prediction intervals in the ARIMA model.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q23">
                <h3>Question 23: What does it mean when an ARIMA model is said to be 'invertible'?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q23" id="q23o0" value="0"><label for="q23o0">The MA component can be rewritten as an infinite AR process</label></div>
<div class="option"><input type="radio" name="q23" id="q23o1" value="1"><label for="q23o1">The model can be solved for both forecasting and backcasting</label></div>
<div class="option"><input type="radio" name="q23" id="q23o2" value="2"><label for="q23o2">The model parameters can be reversed to get the original time series</label></div>
<div class="option"><input type="radio" name="q23" id="q23o3" value="3"><label for="q23o3">The model works equally well on the original and differenced series</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> The MA component can be rewritten as an infinite AR process</p>
                    <p><strong>Explanation:</strong> In time series analysis, when an ARIMA model is said to be 'invertible,' it means that its Moving Average (MA) component can be rewritten as an infinite Autoregressive (AR) process. This property ensures that the MA coefficients decrease in impact as we go further back in time, allowing the process to be approximated by a finite AR model. Invertibility is a mathematical property that ensures a unique MA representation and stable forecasting. Technically, for invertibility, the roots of the MA polynomial must lie outside the unit circle. Without invertibility, different MA models could produce identical autocorrelation patterns, making identification problematic. Invertibility is analogous to stationarity for AR processes and is checked during the model estimation phase.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q24">
                <h3>Question 24: What is the difference between strong and weak stationarity in time series?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q24" id="q24o0" value="0"><label for="q24o0">Weak stationarity requires constant mean and variance and time-invariant autocorrelation; strong stationarity requires the entire distribution to be time-invariant</label></div>
<div class="option"><input type="radio" name="q24" id="q24o1" value="1"><label for="q24o1">Strong stationarity applies to long time series, weak stationarity to short time series</label></div>
<div class="option"><input type="radio" name="q24" id="q24o2" value="2"><label for="q24o2">Strong stationarity means no differencing is required, weak stationarity means first differencing is sufficient</label></div>
<div class="option"><input type="radio" name="q24" id="q24o3" value="3"><label for="q24o3">Weak stationarity allows for seasonal patterns, strong stationarity does not</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Weak stationarity requires constant mean and variance and time-invariant autocorrelation; strong stationarity requires the entire distribution to be time-invariant</p>
                    <p><strong>Explanation:</strong> The distinction between strong (strict) and weak stationarity lies in how much of the data's statistical properties must remain constant over time. Weak stationarity, which is usually sufficient for ARIMA modeling, requires only that the mean and variance remain constant and that the autocorrelation function depends only on the lag between points, not their absolute position in time. In contrast, strong stationarity is more demanding, requiring that the entire joint probability distribution of the process remains unchanged when shifted in time. This means all higher moments (not just the first two) must be constant, and all multivariate distributions (not just bivariate correlations) must be time-invariant. In practice, analysts typically work with weak stationarity because it's easier to test for and sufficient for many applications.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q25">
                <h3>Question 25: What is the primary purpose of the KPSS test in time series analysis?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q25" id="q25o0" value="0"><label for="q25o0">To test for stationarity with a null hypothesis of stationarity</label></div>
<div class="option"><input type="radio" name="q25" id="q25o1" value="1"><label for="q25o1">To determine the optimal order of differencing</label></div>
<div class="option"><input type="radio" name="q25" id="q25o2" value="2"><label for="q25o2">To test for normality of residuals</label></div>
<div class="option"><input type="radio" name="q25" id="q25o3" value="3"><label for="q25o3">To assess the significance of seasonal components</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> To test for stationarity with a null hypothesis of stationarity</p>
                    <p><strong>Explanation:</strong> The KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test is used to test for stationarity in time series analysis, but unlike the ADF test, its null hypothesis is that the series is stationary. This reversal makes it a complementary test to ADF, which has a null hypothesis of non-stationarity. Using both tests together provides stronger evidence: if the ADF test rejects its null and the KPSS fails to reject its null, you have consistent evidence of stationarity. The KPSS test specifically tests whether the series can be described as stationary around a deterministic trend or has a unit root. A low p-value leads to rejecting the null, suggesting non-stationarity. This test is particularly useful for distinguishing between trend-stationary processes and difference-stationary processes.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q26">
                <h3>Question 26: What does Facebook Prophet use to model seasonality in time series data?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q26" id="q26o0" value="0"><label for="q26o0">Fourier series for multiple seasonal periods</label></div>
<div class="option"><input type="radio" name="q26" id="q26o1" value="1"><label for="q26o1">ARMA models with seasonal lags</label></div>
<div class="option"><input type="radio" name="q26" id="q26o2" value="2"><label for="q26o2">Exponential smoothing with seasonal components</label></div>
<div class="option"><input type="radio" name="q26" id="q26o3" value="3"><label for="q26o3">Seasonal dummies for each period</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Fourier series for multiple seasonal periods</p>
                    <p><strong>Explanation:</strong> Facebook Prophet uses Fourier series to model seasonality in time series data. This approach represents seasonal patterns as a sum of sine and cosine terms of different frequencies, allowing for flexible modeling of complex seasonal patterns. Prophet can simultaneously model multiple seasonal periods (e.g., daily, weekly, and yearly seasonality) by using different Fourier series for each. The number of terms in each Fourier series (specified by the 'order' parameter) controls the flexibility of the seasonal component - higher orders capture more complex patterns but risk overfitting. This approach is particularly powerful because it can handle irregular time series and missing data better than traditional seasonal ARIMA models, which require regular time intervals.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q27">
                <h3>Question 27: What are the three main components of a Facebook Prophet model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q27" id="q27o0" value="0"><label for="q27o0">Trend, seasonality, and holidays/events</label></div>
<div class="option"><input type="radio" name="q27" id="q27o1" value="1"><label for="q27o1">Mean, variance, and autocorrelation</label></div>
<div class="option"><input type="radio" name="q27" id="q27o2" value="2"><label for="q27o2">Intercept, slope, and residuals</label></div>
<div class="option"><input type="radio" name="q27" id="q27o3" value="3"><label for="q27o3">AR terms, MA terms, and exogenous variables</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Trend, seasonality, and holidays/events</p>
                    <p><strong>Explanation:</strong> Facebook Prophet decomposes time series into three main components: trend, seasonality, and holidays/events. The trend component captures non-periodic changes, and can be modeled as either linear or logistic growth with automatic changepoint detection to accommodate trend changes. The seasonality component captures periodic patterns using Fourier series, and can simultaneously model multiple seasonal patterns (e.g., daily, weekly, annual). The holidays/events component accounts for irregular schedules and events that affect the time series but don't follow a seasonal pattern. Users can provide a custom list of holidays or events with their dates. By modeling these components separately and then adding them together, Prophet creates an interpretable forecast that can be easily understood and adjusted by analysts.</p>
                    
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
        </div>
    </div>
    <script src="../quiz.js"></script>
</body>
</html>
    