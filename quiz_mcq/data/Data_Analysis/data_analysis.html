
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data_Analysis Quiz</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <div class="container">
        <h1>Data_Analysis Quiz</h1>
        <div class="quiz-container">
    
            <div class="question" id="q1">
                <h3>Question 1: What technique would you use to handle high-dimensional sparse data when performing PCA?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q1" id="q1o0" value="0"><label for="q1o0">Standard PCA with normalization</label></div>
<div class="option"><input type="radio" name="q1" id="q1o1" value="1"><label for="q1o1">`Truncated SVD` (also known as LSA)</label></div>
<div class="option"><input type="radio" name="q1" id="q1o2" value="2"><label for="q1o2">`Kernel PCA` with RBF kernel</label></div>
<div class="option"><input type="radio" name="q1" id="q1o3" value="3"><label for="q1o3">`Factor Analysis`</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Truncated SVD` (also known as LSA)</p>
                    <p><strong>Explanation:</strong> Truncated SVD is specifically designed for sparse matrices and doesn't center the data (which would destroy sparsity), making it more memory-efficient and appropriate for high-dimensional sparse datasets.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_01_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q2">
                <h3>Question 2: What's the most efficient way to perform grouped sampling with replacement in pandas, ensuring each group maintains its original size?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q2" id="q2o0" value="0"><label for="q2o0">`df.groupby('group').apply(lambda x: x.sample(n=len(x), replace=True))`</label></div>
<div class="option"><input type="radio" name="q2" id="q2o1" value="1"><label for="q2o1">`pd.concat([df[df['group']==g].sample(n=sum(df['group']==g), replace=True) for g in df['group'].unique()])`</label></div>
<div class="option"><input type="radio" name="q2" id="q2o2" value="2"><label for="q2o2">`df.set_index('group').sample(frac=1, replace=True).reset_index()`</label></div>
<div class="option"><input type="radio" name="q2" id="q2o3" value="3"><label for="q2o3">`df.groupby('group').apply(lambda x: x.iloc[np.random.choice(len(x), size=len(x), replace=True)])`</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `df.groupby('group').apply(lambda x: x.iloc[np.random.choice(len(x), size=len(x), replace=True)])`</p>
                    <p><strong>Explanation:</strong> This approach uses numpy's efficient random sampling directly on indices, avoiding the overhead of pandas' sample function while maintaining group sizes and allowing replacement.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_02_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q3">
                <h3>Question 3: When implementing stratified k-fold cross-validation for a multi-label classification problem, which approach is most statistically sound?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q3" id="q3o0" value="0"><label for="q3o0">Use sklearn's `StratifiedKFold` with the most common label for each instance</label></div>
<div class="option"><input type="radio" name="q3" id="q3o1" value="1"><label for="q3o1">Create an iterative partitioning algorithm that balances all label combinations across folds</label></div>
<div class="option"><input type="radio" name="q3" id="q3o2" value="2"><label for="q3o2">Use sklearn's `MultilabelStratifiedKFold` from the iterative-stratification package</label></div>
<div class="option"><input type="radio" name="q3" id="q3o3" value="3"><label for="q3o3">Convert to a multi-class problem using label powerset and then apply standard stratification</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Use sklearn's `MultilabelStratifiedKFold` from the iterative-stratification package</p>
                    <p><strong>Explanation:</strong> MultilabelStratifiedKFold implements iterative stratification, which preserves the distribution of all labels across folds, addressing the key challenge in multi-label stratification that normal StratifiedKFold cannot handle.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_03_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q4">
                <h3>Question 4: Which approach correctly calculates the Wasserstein distance (Earth Mover's Distance) between two empirical distributions in Python?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q4" id="q4o0" value="0"><label for="q4o0">`scipy.stats.wasserstein_distance(x, y)`</label></div>
<div class="option"><input type="radio" name="q4" id="q4o1" value="1"><label for="q4o1">`numpy.linalg.norm(np.sort(x) - np.sort(y), ord=1)`</label></div>
<div class="option"><input type="radio" name="q4" id="q4o2" value="2"><label for="q4o2">`scipy.spatial.distance.cdist(x.reshape(-1,1), y.reshape(-1,1), metric='euclidean').min(axis=1).sum()`</label></div>
<div class="option"><input type="radio" name="q4" id="q4o3" value="3"><label for="q4o3">`sklearn.metrics.pairwise_distances(x.reshape(-1,1), y.reshape(-1,1), metric='manhattan').min(axis=1).mean()`</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `scipy.stats.wasserstein_distance(x, y)`</p>
                    <p><strong>Explanation:</strong> `scipy.stats.wasserstein_distance` correctly implements the 1D Wasserstein distance between empirical distributions, which measures the minimum 'work' required to transform one distribution into another.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_04_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q5">
                <h3>Question 5: What's the most computationally efficient way to find the k-nearest neighbors for each point in a large dataset using scikit-learn?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q5" id="q5o0" value="0"><label for="q5o0">`sklearn.neighbors.NearestNeighbors(n_neighbors=k, algorithm='brute').fit(X).kneighbors(X)`</label></div>
<div class="option"><input type="radio" name="q5" id="q5o1" value="1"><label for="q5o1">`sklearn.neighbors.NearestNeighbors(n_neighbors=k, algorithm='kd_tree').fit(X).kneighbors(X)`</label></div>
<div class="option"><input type="radio" name="q5" id="q5o2" value="2"><label for="q5o2">`sklearn.neighbors.NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X).kneighbors(X)`</label></div>
<div class="option"><input type="radio" name="q5" id="q5o3" value="3"><label for="q5o3">Depends on data dimensionality, size, and structure</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Depends on data dimensionality, size, and structure</p>
                    <p><strong>Explanation:</strong> The most efficient algorithm depends on the dataset characteristics: brute force works well for small datasets and high dimensions, kd_tree excels in low dimensions (<20), and ball_tree performs better in higher dimensions or with non-Euclidean metrics.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_05_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q6">
                <h3>Question 6: When dealing with millions of rows of time series data with irregular timestamps, which method is most efficient for resampling to regular intervals with proper handling of missing values?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q6" id="q6o0" value="0"><label for="q6o0">`df.set_index('timestamp').asfreq('1H').interpolate(method='time')`</label></div>
<div class="option"><input type="radio" name="q6" id="q6o1" value="1"><label for="q6o1">`df.set_index('timestamp').resample('1H').asfreq().interpolate(method='time')`</label></div>
<div class="option"><input type="radio" name="q6" id="q6o2" value="2"><label for="q6o2">`df.set_index('timestamp').resample('1H').mean().interpolate(method='time')`</label></div>
<div class="option"><input type="radio" name="q6" id="q6o3" value="3"><label for="q6o3">`df.groupby(pd.Grouper(key='timestamp', freq='1H')).apply(lambda x: x.mean() if not x.empty else pd.Series(np.nan, index=df.columns))`</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `df.set_index('timestamp').resample('1H').asfreq().interpolate(method='time')`</p>
                    <p><strong>Explanation:</strong> This approach correctly converts irregular timestamps to a regular frequency with .resample('1H').asfreq(), then intelligently fills missing values using time-based interpolation which respects the actual timing of observations.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_06_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q7">
                <h3>Question 7: Which technique is most appropriate for identifying non-linear relationships between variables in a high-dimensional dataset?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q7" id="q7o0" value="0"><label for="q7o0">Pearson correlation matrix with hierarchical clustering</label></div>
<div class="option"><input type="radio" name="q7" id="q7o1" value="1"><label for="q7o1">Distance correlation matrix with MDS visualization</label></div>
<div class="option"><input type="radio" name="q7" id="q7o2" value="2"><label for="q7o2">`MINE` statistics (Maximal Information-based Nonparametric Exploration)</label></div>
<div class="option"><input type="radio" name="q7" id="q7o3" value="3"><label for="q7o3">Random Forest feature importance with partial dependence plots</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `MINE` statistics (Maximal Information-based Nonparametric Exploration)</p>
                    <p><strong>Explanation:</strong> MINE statistics, particularly the Maximal Information Coefficient (MIC), detect both linear and non-linear associations without assuming a specific functional form, outperforming traditional correlation measures for complex relationships.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_07_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q8">
                <h3>Question 8: What's the most statistically sound approach to handle imbalanced multiclass classification with severe class imbalance?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q8" id="q8o0" value="0"><label for="q8o0">Oversampling minority classes using SMOTE</label></div>
<div class="option"><input type="radio" name="q8" id="q8o1" value="1"><label for="q8o1">Undersampling majority classes using NearMiss</label></div>
<div class="option"><input type="radio" name="q8" id="q8o2" value="2"><label for="q8o2">Cost-sensitive learning with class weights inversely proportional to frequencies</label></div>
<div class="option"><input type="radio" name="q8" id="q8o3" value="3"><label for="q8o3">Ensemble of balanced subsets with `META` learning</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Ensemble of balanced subsets with `META` learning</p>
                    <p><strong>Explanation:</strong> META (Minority Ethnicity and Threshold Adjustment) learning with ensembling addresses severe multiclass imbalance by training multiple models on balanced subsets and combining them, avoiding information loss from undersampling while preventing the artificial patterns that can be introduced by synthetic oversampling.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_08_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q9">
                <h3>Question 9: What's the correct approach to implement a memory-efficient pipeline for one-hot encoding categorical variables with high cardinality in pandas?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q9" id="q9o0" value="0"><label for="q9o0">`pd.get_dummies(df, sparse=True)`</label></div>
<div class="option"><input type="radio" name="q9" id="q9o1" value="1"><label for="q9o1">`pd.Categorical(df['col']).codes` in combination with sklearn's `OneHotEncoder(sparse=True)`</label></div>
<div class="option"><input type="radio" name="q9" id="q9o2" value="2"><label for="q9o2">Use `pd.factorize()` on all categorical columns followed by scipy's sparse matrices</label></div>
<div class="option"><input type="radio" name="q9" id="q9o3" value="3"><label for="q9o3">Convert to category dtype then use `df['col'].cat.codes` with sklearn's `OneHotEncoder(sparse=True)`</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Convert to category dtype then use `df['col'].cat.codes` with sklearn's `OneHotEncoder(sparse=True)`</p>
                    <p><strong>Explanation:</strong> Converting to pandas' memory-efficient category dtype first, then using cat.codes with a sparse OneHotEncoder creates a memory-efficient pipeline that preserves category labels and works well with scikit-learn while minimizing memory usage.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_09_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q10">
                <h3>Question 10: Which approach correctly implements a multi-output Gradient Boosting Regressor for simultaneously predicting multiple continuous targets with different scales?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q10" id="q10o0" value="0"><label for="q10o0">`MultiOutputRegressor(GradientBoostingRegressor())`</label></div>
<div class="option"><input type="radio" name="q10" id="q10o1" value="1"><label for="q10o1">`GradientBoostingRegressor` with `multioutput='raw_values'`</label></div>
<div class="option"><input type="radio" name="q10" id="q10o2" value="2"><label for="q10o2">`RegressorChain(GradientBoostingRegressor())` with StandardScaler for each target</label></div>
<div class="option"><input type="radio" name="q10" id="q10o3" value="3"><label for="q10o3">Separate scaled `GradientBoostingRegressor` for each target in a Pipeline</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `MultiOutputRegressor(GradientBoostingRegressor())`</p>
                    <p><strong>Explanation:</strong> MultiOutputRegressor fits a separate GradientBoostingRegressor for each target, allowing each model to optimize independently, which is crucial when targets have different scales and relationships with features.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_10_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q11">
                <h3>Question 11: When performing anomaly detection in a multivariate time series, which technique is most appropriate for detecting contextual anomalies?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q11" id="q11o0" value="0"><label for="q11o0">`Isolation Forest` with sliding windows</label></div>
<div class="option"><input type="radio" name="q11" id="q11o1" value="1"><label for="q11o1">`One-class SVM` on feature vectors</label></div>
<div class="option"><input type="radio" name="q11" id="q11o2" value="2"><label for="q11o2">`LSTM Autoencoder` with reconstruction error thresholding</label></div>
<div class="option"><input type="radio" name="q11" id="q11o3" value="3"><label for="q11o3">`ARIMA` with Mahalanobis distance on residuals</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `LSTM Autoencoder` with reconstruction error thresholding</p>
                    <p><strong>Explanation:</strong> LSTM Autoencoders can capture complex temporal dependencies in multivariate time series data, making them ideal for detecting contextual anomalies where data points are abnormal specifically in their context rather than globally.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_11_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q12">
                <h3>Question 12: What's the most rigorous approach to perform causal inference from observational data when randomized experiments aren't possible?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q12" id="q12o0" value="0"><label for="q12o0">Propensity score matching with sensitivity analysis</label></div>
<div class="option"><input type="radio" name="q12" id="q12o1" value="1"><label for="q12o1">Instrumental variable analysis with validity tests</label></div>
<div class="option"><input type="radio" name="q12" id="q12o2" value="2"><label for="q12o2">Causal graphical models with do-calculus</label></div>
<div class="option"><input type="radio" name="q12" id="q12o3" value="3"><label for="q12o3">Difference-in-differences with parallel trends validation</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Causal graphical models with do-calculus</p>
                    <p><strong>Explanation:</strong> Causal graphical models using do-calculus provide a comprehensive mathematical framework for identifying causal effects from observational data, allowing researchers to formally express causal assumptions and determine whether causal quantities are identifiable from available data.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_12_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q13">
                <h3>Question 13: Which technique is most appropriate for efficiently clustering a dataset with millions of data points and hundreds of features?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q13" id="q13o0" value="0"><label for="q13o0">`Mini-batch K-means` with dimensionality reduction</label></div>
<div class="option"><input type="radio" name="q13" id="q13o1" value="1"><label for="q13o1">`HDBSCAN` with feature selection</label></div>
<div class="option"><input type="radio" name="q13" id="q13o2" value="2"><label for="q13o2">`Birch` (Balanced Iterative Reducing and Clustering using Hierarchies)</label></div>
<div class="option"><input type="radio" name="q13" id="q13o3" value="3"><label for="q13o3">`Spectral clustering` with NystrÃ¶m approximation</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Birch` (Balanced Iterative Reducing and Clustering using Hierarchies)</p>
                    <p><strong>Explanation:</strong> Birch is specifically designed for very large datasets as it builds a tree structure in a single pass through the data, has linear time complexity, limited memory requirements, and can handle outliers effectively, making it ideal for clustering massive high-dimensional datasets.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_13_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q14">
                <h3>Question 14: What's the most rigorous method for selecting the optimal number of components in a Gaussian Mixture Model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q14" id="q14o0" value="0"><label for="q14o0">Elbow method with distortion scores</label></div>
<div class="option"><input type="radio" name="q14" id="q14o1" value="1"><label for="q14o1">Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC)</label></div>
<div class="option"><input type="radio" name="q14" id="q14o2" value="2"><label for="q14o2">Cross-validation with log-likelihood scoring</label></div>
<div class="option"><input type="radio" name="q14" id="q14o3" value="3"><label for="q14o3">Variational Bayesian inference with automatic relevance determination</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Variational Bayesian inference with automatic relevance determination</p>
                    <p><strong>Explanation:</strong> Variational Bayesian inference with automatic relevance determination (implemented in sklearn as GaussianMixture(n_components=n, weight_concentration_prior_type='dirichlet_process')) can automatically prune unnecessary components, effectively determining the optimal number without requiring multiple model fits and comparisons.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_14_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q15">
                <h3>Question 15: What's the correct approach to implement a custom scoring function for model evaluation in scikit-learn that handles class imbalance better than accuracy?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q15" id="q15o0" value="0"><label for="q15o0">`sklearn.metrics.make_scorer(custom_metric, greater_is_better=True)`</label></div>
<div class="option"><input type="radio" name="q15" id="q15o1" value="1"><label for="q15o1">`sklearn.metrics.make_scorer(custom_metric, needs_proba=True, greater_is_better=True)`</label></div>
<div class="option"><input type="radio" name="q15" id="q15o2" value="2"><label for="q15o2">Create a scorer class that implements __call__(self, estimator, X, y) and gets_score() methods</label></div>
<div class="option"><input type="radio" name="q15" id="q15o3" value="3"><label for="q15o3">A and B are both correct depending on the custom_metric function</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> A and B are both correct depending on the custom_metric function</p>
                    <p><strong>Explanation:</strong> make_scorer() is the correct approach, but the parameters depend on the specific metric: needs_proba=True for metrics requiring probability estimates (like AUC), and needs_threshold=True for metrics requiring decision thresholds; the appropriate configuration varies based on the specific imbalance-handling metric.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_15_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q16">
                <h3>Question 16: Which approach correctly implements a memory-efficient data pipeline for processing and analyzing a dataset too large to fit in memory?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q16" id="q16o0" value="0"><label for="q16o0">Use pandas with `low_memory=True` and `chunksize` parameter</label></div>
<div class="option"><input type="radio" name="q16" id="q16o1" value="1"><label for="q16o1">Implement `dask.dataframe` with lazy evaluation and out-of-core computation</label></div>
<div class="option"><input type="radio" name="q16" id="q16o2" value="2"><label for="q16o2">Use pandas-on-spark (formerly Koalas) with distributed processing</label></div>
<div class="option"><input type="radio" name="q16" id="q16o3" value="3"><label for="q16o3">Implement `vaex` for memory-mapping and out-of-core dataframes</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Implement `dask.dataframe` with lazy evaluation and out-of-core computation</p>
                    <p><strong>Explanation:</strong> dask.dataframe provides a pandas-like API with lazy evaluation, parallel execution, and out-of-core computation, allowing for scalable data processing beyond available RAM while maintaining familiar pandas operations and requiring minimal code changes.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_16_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q17">
                <h3>Question 17: When performing hyperparameter tuning for a complex model with many parameters, which advanced optimization technique is most efficient?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q17" id="q17o0" value="0"><label for="q17o0">Random search with early stopping</label></div>
<div class="option"><input type="radio" name="q17" id="q17o1" value="1"><label for="q17o1">Genetic algorithms with tournament selection</label></div>
<div class="option"><input type="radio" name="q17" id="q17o2" value="2"><label for="q17o2">Bayesian optimization with Gaussian processes</label></div>
<div class="option"><input type="radio" name="q17" id="q17o3" value="3"><label for="q17o3">Hyperband with successive halving</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Bayesian optimization with Gaussian processes</p>
                    <p><strong>Explanation:</strong> Bayesian optimization with Gaussian processes builds a probabilistic model of the objective function to intelligently select the most promising hyperparameter configurations based on previous evaluations, making it more efficient than random or grid search for exploring high-dimensional parameter spaces.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_17_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q18">
                <h3>Question 18: What's the most statistically sound approach to handle heteroscedasticity in a regression model?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q18" id="q18o0" value="0"><label for="q18o0">Visual inspection of residuals vs. fitted values plot</label></div>
<div class="option"><input type="radio" name="q18" id="q18o1" value="1"><label for="q18o1">`Breusch-Pagan` test for constant variance</label></div>
<div class="option"><input type="radio" name="q18" id="q18o2" value="2"><label for="q18o2">`White's test` for homoscedasticity</label></div>
<div class="option"><input type="radio" name="q18" id="q18o3" value="3"><label for="q18o3">Both B and C, with different null hypotheses</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Both B and C, with different null hypotheses</p>
                    <p><strong>Explanation:</strong> Both tests detect heteroscedasticity but with different assumptions: Breusch-Pagan assumes that heteroscedasticity is a linear function of the independent variables, while White's test is more general and doesn't make this assumption, making them complementary approaches.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_18_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q19">
                <h3>Question 19: Which approach correctly implements a hierarchical time series forecasting model that respects aggregation constraints?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q19" id="q19o0" value="0"><label for="q19o0">Bottom-up approach: forecast at lowest level and aggregate upwards</label></div>
<div class="option"><input type="radio" name="q19" id="q19o1" value="1"><label for="q19o1">Top-down approach: forecast at highest level and disaggregate proportionally</label></div>
<div class="option"><input type="radio" name="q19" id="q19o2" value="2"><label for="q19o2">Middle-out approach: forecast at a middle level and propagate in both directions</label></div>
<div class="option"><input type="radio" name="q19" id="q19o3" value="3"><label for="q19o3">Reconciliation approach: forecast at all levels independently then reconcile with constraints</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Reconciliation approach: forecast at all levels independently then reconcile with constraints</p>
                    <p><strong>Explanation:</strong> The reconciliation approach (optimal combination) generates forecasts at all levels independently, then applies a mathematical reconciliation procedure that minimizes revisions while ensuring hierarchical consistency, typically outperforming both bottom-up and top-down approaches.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_19_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q20">
                <h3>Question 20: What technique is most appropriate for analyzing complex network data with community structures?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q20" id="q20o0" value="0"><label for="q20o0">K-means clustering on the adjacency matrix</label></div>
<div class="option"><input type="radio" name="q20" id="q20o1" value="1"><label for="q20o1">Spectral clustering with normalized Laplacian</label></div>
<div class="option"><input type="radio" name="q20" id="q20o2" value="2"><label for="q20o2">`Louvain` algorithm for community detection</label></div>
<div class="option"><input type="radio" name="q20" id="q20o3" value="3"><label for="q20o3">DBSCAN on node2vec embeddings</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Louvain` algorithm for community detection</p>
                    <p><strong>Explanation:</strong> The Louvain algorithm specifically optimizes modularity to detect communities in networks, automatically finding the appropriate number of communities and handling multi-scale resolution, making it ideal for complex networks with hierarchical community structures.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_20_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q21">
                <h3>Question 21: What's the most robust approach to handle concept drift in a production machine learning system?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q21" id="q21o0" value="0"><label for="q21o0">Implement automatic model retraining when performance degrades below a threshold</label></div>
<div class="option"><input type="radio" name="q21" id="q21o1" value="1"><label for="q21o1">Use an ensemble of models with different time windows</label></div>
<div class="option"><input type="radio" name="q21" id="q21o2" value="2"><label for="q21o2">Implement drift detection algorithms with adaptive learning techniques</label></div>
<div class="option"><input type="radio" name="q21" id="q21o3" value="3"><label for="q21o3">Deploy a champion-challenger framework with continuous evaluation</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Implement drift detection algorithms with adaptive learning techniques</p>
                    <p><strong>Explanation:</strong> This approach combines statistical drift detection (e.g., ADWIN, DDM, or KSWIN) with adaptive learning methods that can continuously update models or model weights as new patterns emerge, allowing for immediate adaptation to changing data distributions.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_21_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q22">
                <h3>Question 22: Which method is most appropriate for interpretable anomaly detection in high-dimensional data?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q22" id="q22o0" value="0"><label for="q22o0">`Isolation Forest` with LIME explanations</label></div>
<div class="option"><input type="radio" name="q22" id="q22o1" value="1"><label for="q22o1">Autoencoders with attention mechanisms</label></div>
<div class="option"><input type="radio" name="q22" id="q22o2" value="2"><label for="q22o2">SHAP values on `One-Class SVM` predictions</label></div>
<div class="option"><input type="radio" name="q22" id="q22o3" value="3"><label for="q22o3">Supervised anomaly detection with feature importance</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Isolation Forest` with LIME explanations</p>
                    <p><strong>Explanation:</strong> Isolation Forest efficiently detects anomalies in high dimensions by isolating observations, while LIME provides local interpretable explanations for each anomaly, showing which features contributed most to its identification, making the detection both efficient and explainable.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_22_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q23">
                <h3>Question 23: When implementing a multi-armed bandit algorithm for real-time optimization, which approach balances exploration and exploitation most effectively?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q23" id="q23o0" value="0"><label for="q23o0">Epsilon-greedy with annealing schedule</label></div>
<div class="option"><input type="radio" name="q23" id="q23o1" value="1"><label for="q23o1">`Upper Confidence Bound` (UCB) algorithm</label></div>
<div class="option"><input type="radio" name="q23" id="q23o2" value="2"><label for="q23o2">`Thompson Sampling` with prior distribution updates</label></div>
<div class="option"><input type="radio" name="q23" id="q23o3" value="3"><label for="q23o3">Contextual bandits with linear payoffs</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Thompson Sampling` with prior distribution updates</p>
                    <p><strong>Explanation:</strong> Thompson Sampling with Bayesian updates to prior distributions maintains explicit uncertainty estimates and naturally balances exploration/exploitation, with theoretical guarantees of optimality and empirically better performance than UCB and epsilon-greedy methods in many applications.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_23_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q24">
                <h3>Question 24: What's the most efficient technique for calculating pairwise distances between all points in a very large dataset?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q24" id="q24o0" value="0"><label for="q24o0">`numpy.linalg.norm` with broadcasting</label></div>
<div class="option"><input type="radio" name="q24" id="q24o1" value="1"><label for="q24o1">`scipy.spatial.distance.pdist` with `squareform`</label></div>
<div class="option"><input type="radio" name="q24" id="q24o2" value="2"><label for="q24o2">`sklearn.metrics.pairwise_distances` with `n_jobs=-1`</label></div>
<div class="option"><input type="radio" name="q24" id="q24o3" value="3"><label for="q24o3">Custom `numba`-accelerated function with parallel processing</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `scipy.spatial.distance.pdist` with `squareform`</p>
                    <p><strong>Explanation:</strong> pdist computes distances using an optimized implementation that avoids redundant calculations (since distance matrices are symmetric), and squareform can convert to a square matrix if needed; this approach is significantly more memory-efficient than computing the full distance matrix directly.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_24_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q25">
                <h3>Question 25: Which method is most appropriate for detecting and handling multivariate outliers in high-dimensional data?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q25" id="q25o0" value="0"><label for="q25o0">Z-scores on each dimension independently</label></div>
<div class="option"><input type="radio" name="q25" id="q25o1" value="1"><label for="q25o1">`Mahalanobis distance` with robust covariance estimation</label></div>
<div class="option"><input type="radio" name="q25" id="q25o2" value="2"><label for="q25o2">`Local Outlier Factor` with appropriate neighborhood size</label></div>
<div class="option"><input type="radio" name="q25" id="q25o3" value="3"><label for="q25o3">`Isolation Forest` with random projection</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Mahalanobis distance` with robust covariance estimation</p>
                    <p><strong>Explanation:</strong> Mahalanobis distance accounts for the covariance structure of the data, and using robust covariance estimation (e.g., Minimum Covariance Determinant) prevents outliers from influencing the distance metric itself, making it ideal for identifying multivariate outliers.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_25_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q26">
                <h3>Question 26: What's the most appropriate technique for feature selection when dealing with multicollinearity in a regression context?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q26" id="q26o0" value="0"><label for="q26o0">Forward stepwise selection with VIF thresholding</label></div>
<div class="option"><input type="radio" name="q26" id="q26o1" value="1"><label for="q26o1">`Elastic Net` regularization with cross-validation</label></div>
<div class="option"><input type="radio" name="q26" id="q26o2" value="2"><label for="q26o2">Principal Component Regression (PCR)</label></div>
<div class="option"><input type="radio" name="q26" id="q26o3" value="3"><label for="q26o3">Recursive Feature Elimination with stability selection</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Elastic Net` regularization with cross-validation</p>
                    <p><strong>Explanation:</strong> Elastic Net combines L1 and L2 penalties, handling multicollinearity by grouping correlated features while still performing feature selection, with the optimal balance determined through cross-validationâ€”making it more effective than methods that either eliminate or transform features.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_26_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q27">
                <h3>Question 27: Which approach correctly implements online learning for a classification task with a non-stationary data distribution?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q27" id="q27o0" value="0"><label for="q27o0">`SGDClassifier` with `partial_fit` and appropriate `class_weight` adjustments</label></div>
<div class="option"><input type="radio" name="q27" id="q27o1" value="1"><label for="q27o1">`River's HoeffdingTreeClassifier` with drift detection</label></div>
<div class="option"><input type="radio" name="q27" id="q27o2" value="2"><label for="q27o2">Custom implementation using incremental learning and time-based feature weighting</label></div>
<div class="option"><input type="radio" name="q27" id="q27o3" value="3"><label for="q27o3">Ensemble of incremental learners with dynamic weighting based on recent performance</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Ensemble of incremental learners with dynamic weighting based on recent performance</p>
                    <p><strong>Explanation:</strong> This ensemble approach maintains multiple incremental models updated with new data, dynamically adjusting their weights based on recent performance, allowing the system to adapt to concept drift by giving more influence to models that perform well on recent data.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_27_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q28">
                <h3>Question 28: What's the most rigorous approach to handle missing data in a longitudinal study with potential non-random missingness?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q28" id="q28o0" value="0"><label for="q28o0">Multiple imputation by chained equations (MICE) with auxiliary variables</label></div>
<div class="option"><input type="radio" name="q28" id="q28o1" value="1"><label for="q28o1">Pattern mixture models with sensitivity analysis</label></div>
<div class="option"><input type="radio" name="q28" id="q28o2" value="2"><label for="q28o2">Joint modeling of missingness and outcomes</label></div>
<div class="option"><input type="radio" name="q28" id="q28o3" value="3"><label for="q28o3">Inverse probability weighting with doubly robust estimation</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> Joint modeling of missingness and outcomes</p>
                    <p><strong>Explanation:</strong> Joint modeling directly incorporates the missingness mechanism into the analysis model, allowing for valid inference under non-random missingness (MNAR) scenarios by explicitly modeling the relationship between the missing data process and the outcomes of interest.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_28_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q29">
                <h3>Question 29: Which technique is most appropriate for analyzing complex interactions between variables in a predictive modeling context?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q29" id="q29o0" value="0"><label for="q29o0">Generalized Additive Models with tensor product smooths</label></div>
<div class="option"><input type="radio" name="q29" id="q29o1" value="1"><label for="q29o1">`Random Forest` with partial dependence plots and ICE curves</label></div>
<div class="option"><input type="radio" name="q29" id="q29o2" value="2"><label for="q29o2">`Neural networks` with feature crossing and attention mechanisms</label></div>
<div class="option"><input type="radio" name="q29" id="q29o3" value="3"><label for="q29o3">`Gradient Boosting` with SHAP interaction values</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Gradient Boosting` with SHAP interaction values</p>
                    <p><strong>Explanation:</strong> Gradient Boosting effectively captures complex non-linear relationships, while SHAP interaction values specifically quantify how much of the prediction is attributable to interactions between features, providing a rigorous statistical framework for analyzing and visualizing interactions.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_29_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
            <div class="question" id="q30">
                <h3>Question 30: What's the most statistically sound approach to perform feature selection for a regression task with potential non-linear relationships?</h3>
                <div class="options">
                    <div class="option"><input type="radio" name="q30" id="q30o0" value="0"><label for="q30o0">`Mutual information`-based selection with permutation testing</label></div>
<div class="option"><input type="radio" name="q30" id="q30o1" value="1"><label for="q30o1">`LASSO` regression with stability selection</label></div>
<div class="option"><input type="radio" name="q30" id="q30o2" value="2"><label for="q30o2">`Random Forest` with Boruta algorithm</label></div>
<div class="option"><input type="radio" name="q30" id="q30o3" value="3"><label for="q30o3">`Generalized Additive Models` with significance testing of smooth terms</label></div>
                </div>
                <div class="answer hidden">
                    <p><strong>Answer:</strong> `Mutual information`-based selection with permutation testing</p>
                    <p><strong>Explanation:</strong> Mutual information captures both linear and non-linear dependencies between variables without assuming functional form, while permutation testing provides a statistically rigorous way to assess the significance of these dependencies, controlling for multiple testing issues.</p>
                    <p class="long-answer-ref"><a href="../data/Data_Analysis/qn_30_answer_long_01.md" target="_blank">Detailed Explanation</a></p>
                </div>
                <button class="show-answer">Show Answer</button>
            </div>
        
        </div>
    </div>
    <script src="../quiz.js"></script>
</body>
</html>
    